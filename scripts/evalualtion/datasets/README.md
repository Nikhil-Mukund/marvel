## RAGAS-based evaluation datasets (JSONL)

These files provide **per-sample RAGAS metrics** alongside the corresponding QA example and model answers.
All files are newline-delimited JSON (`.jsonl`), with **one JSON object per line** (UTF-8).

### Files

**Full sets (Baseline vs MARVEL-Standard)**  
- `eval_ArxivData_gpt4o-mini_vs_MARVEL-Standard.jsonl` — **N = 910** (Arxiv-derived set; renamed from internal label `LatexData`)  
- `eval_LogbookData_gpt4o-mini_vs_MARVEL-Standard.jsonl` — **N = 696**

**DeepSearch subset (Baseline vs MARVEL-Standard vs MARVEL-DeepSearch)**  
- `eval_ArxivData_gpt4o-mini_vs_MARVEL-Standard_vs_MARVEL-DeepSearch.jsonl` — **N = 168**  
- `eval_LogbookData_gpt4o-mini_vs_MARVEL-Standard_vs_MARVEL-DeepSearch.jsonl` — **N = 135**

### Record schema

Each record contains the QA sample (question + context + reference answer), the model answers, and then RAGAS metrics
computed for each answer.

RAGAS metric values are typically floats in **[0, 1]** (higher is better). Some values may be `null` if a metric could not be computed.

#### Core QA + model fields (present in all files)

Fields are written in a consistent order:

| Field | Type | Description |
|------|------|-------------|
| `dataset_type` | string | Dataset identifier (`"ArxivData"` or `"LogbookData"`). |
| `question` | string | Question to be answered. |
| `context` | string | Context passage associated with the question. Included for reproducibility and analysis. |
| `ground_truth` | string | Reference answer for the question. |
| `gpt4o_mini_answer` | string | Baseline answer generated by OpenAI `gpt-4o-mini`. |
| `gpt4o_mini_score` | int | LLM-based evaluation score for the baseline answer (integer in **[0, 10]**; higher is better). |
| `marvel_standard_answer` | string | Answer generated by **MARVEL-Standard**. |
| `marvel_standard_score` | int | LLM-based evaluation score for the MARVEL-Standard answer (integer in **[0, 10]**; higher is better). |
| `evaluation_model_type` | string | Provider/family of the evaluation model used for the `*_score` fields (e.g., `"openai"`). |
| `evaluation_model_name` | string | Model used to assign the `*_score` fields. |
| `qa_model_type` | string | Provider/family of the model used to generate the QA pair (question + reference). |
| `qa_model_name` | string | Model used to generate the QA pair (question + reference). |

#### Additional QA + model fields (DeepSearch subset files only)

| Field | Type | Description |
|------|------|-------------|
| `marvel_deepsearch_answer` | string | Answer generated by **MARVEL-DeepSearch**. |
| `marvel_deepsearch_score` | int | LLM-based evaluation score for the MARVEL-DeepSearch answer (integer in **[0, 10]**; higher is better). |

### RAGAS metric fields

RAGAS metrics are provided per answer (baseline / MARVEL-Standard / DeepSearch where available):

| Field | Type | Description |
|------|------|-------------|
| `gpt4o_mini_answer_relevancy` | float / null | RAGAS answer relevancy for the baseline answer. |
| `gpt4o_mini_answer_correctness` | float / null | RAGAS answer correctness for the baseline answer. |
| `gpt4o_mini_faithfulness` | float / null | RAGAS faithfulness for the baseline answer. |
| `marvel_standard_answer_relevancy` | float / null | RAGAS answer relevancy for the MARVEL-Standard answer. |
| `marvel_standard_answer_correctness` | float / null | RAGAS answer correctness for the MARVEL-Standard answer. |
| `marvel_standard_faithfulness` | float / null | RAGAS faithfulness for the MARVEL-Standard answer. |

DeepSearch subset files additionally include:

| Field | Type | Description |
|------|------|-------------|
| `marvel_deepsearch_answer_relevancy` | float / null | RAGAS answer relevancy for the MARVEL-DeepSearch answer. |
| `marvel_deepsearch_answer_correctness` | float / null | RAGAS answer correctness for the MARVEL-DeepSearch answer. |
| `marvel_deepsearch_faithfulness` | float / null | RAGAS faithfulness for the MARVEL-DeepSearch answer. |

### Example record (single line)

Example from a DeepSearch subset file (the full-set files omit the `marvel_deepsearch_*` fields):

```json
{
  "dataset_type": "ArxivData",
  "question": "...",
  "context": "...",
  "ground_truth": "...",
  "gpt4o_mini_answer": "...",
  "gpt4o_mini_score": 8,
  "marvel_standard_answer": "...",
  "marvel_standard_score": 7,
  "marvel_deepsearch_answer": "...",
  "marvel_deepsearch_score": 9,
  "evaluation_model_type": "openai",
  "evaluation_model_name": "...",
  "qa_model_type": "...",
  "qa_model_name": "...",
  "gpt4o_mini_answer_relevancy": 0.83,
  "gpt4o_mini_answer_correctness": 0.72,
  "gpt4o_mini_faithfulness": 0.90,
  "marvel_standard_answer_relevancy": 0.87,
  "marvel_standard_answer_correctness": 0.78,
  "marvel_standard_faithfulness": 0.92,
  "marvel_deepsearch_answer_relevancy": 0.89,
  "marvel_deepsearch_answer_correctness": 0.81,
  "marvel_deepsearch_faithfulness": 0.94
}
```
